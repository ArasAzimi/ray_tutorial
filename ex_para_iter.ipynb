{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Guided Tour of Ray Core: Parallel Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*Parallel Iterators*](https://docs.ray.io/en/latest/iter.html) provide a simple yet powerful API for data ingest and stream processing, where transformations are based on method chaining.\n",
    "\n",
    "Parallel iterators get partitioned into *data shards*, and Ray creates a worker (an *actor*) to produces the data for each shard.\n",
    "Evaluation is *lazy*, i.e., only executed when the application calls `next()` to fetch the next item in a sequence.\n",
    "\n",
    "Parallel iterators are fully serializable, so they can be passed to remote tasks and actors.\n",
    "In effect, these can be used to operate over infinite sequences of items, with the processing distributed across a cluster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start Ray…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "import logging\n",
    "import ray\n",
    "\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    logging_level=logging.ERROR,\n",
    ")\n",
    "\n",
    "print(f\"Dashboard URL: http://{ray.get_dashboard_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a parallel iterator from the sequence `items`, using 2 worker actors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [1, 2, 3, 4, 5]\n",
    "\n",
    "iter1 = ray.util.iter.from_items(items, num_shards=2)\n",
    "iter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `iter1` object can now be passed (i.e., serialized) to remote tasks and remote methods.\n",
    "\n",
    "To read elements from a parallel iterator, it can be converted to a [`LocalIterator`](https://docs.ray.io/en/latest/iter.html#ray.util.iter.LocalIterator) using two approaches.\n",
    "\n",
    "Calling [`gather_sync()`](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIterator.gather_sync)\n",
    "returns a local iterable for *synchronous* iteration.\n",
    "In other words, next items will be fetched from the shards on-demand as the application steps through the iterator sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_iter1 = iter1.gather_sync()\n",
    "local_iter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in local_iter1:\n",
    "    ic(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying a function to the sequence (i.e., some kind of transformation) a parallel iterator provides semantic guarantees for *fetch ordering*. In other words, the transformation is guaranteed to get applied to each element of the sequence before the next item is fetched from the source actor.\n",
    "For example, this can be useful if you need to update the source actor between iterator steps.\n",
    "\n",
    "To illustrate a simple case of how to apply a function, first we'll define a class to perform some calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CumulativeSum:\n",
    "    def __init__ (self):\n",
    "        self.total = 0\n",
    "\n",
    "    def __call__ (self, x):\n",
    "        self.total += x\n",
    "        return (self.total, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply that class to the sequence of items, using the [`for_each()`](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIterator.for_each) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in iter1.for_each(CumulativeSum()).gather_sync():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, calling [`gather_async()`](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIterator.gather_async)\n",
    "returns a local iterable for *asynchronous* iteration.\n",
    "In other words, next items will be fetched from the shards asynchronously as soon as the previous item gets computed.\n",
    "In this case, the fetch ordering only applies per shard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to access a parallel iterator is as a collection of its shards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1.shards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shard in iter1.shards():\n",
    "    ic(shard)\n",
    "    \n",
    "    for item in shard:\n",
    "        ic(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each shard should only be read by one process at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's iterate through the JSON source for the Jupyter notebooks in this repo as if this were a streaming input source.\n",
    "\n",
    "We'll filter to get the text in markdown cells, evaluated in batches – which creates a sliding window across the input stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "nb_items = list(Path(\".\").glob(\"ex_*.ipynb\"))\n",
    "window_width = 20\n",
    "\n",
    "iter2 = (\n",
    "    ray.util.iter.from_items(nb_items, num_shards=3)\n",
    "        .for_each(lambda f: json.load(open(f)))\n",
    "        .for_each(lambda nb: nb[\"cells\"])\n",
    "        .flatten()\n",
    "        .for_each(lambda cell: cell[\"source\"] if cell[\"cell_type\"] == \"markdown\" else [])\n",
    "        .flatten()\n",
    "        .for_each(lambda line: 1 if \"Ray\" in line else 0)\n",
    "        .batch(window_width)\n",
    "        .for_each(np.mean)\n",
    ")\n",
    "\n",
    "iter2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the probability of the term `Ray` occurring within the lines in each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for freq in iter2.gather_async():\n",
    "    ic(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rework this to show an example of passing iterator shards to remote functions.\n",
    "We'll define a remote function `nb_word_count` to tally *word count* among the markdown cells in each notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "@ray.remote\n",
    "def nb_word_count (shard):\n",
    "    wc = defaultdict(int)\n",
    "    punct = \"\"\"'`<>[](){}*.,:…-'\"\"\"\n",
    "    \n",
    "    for nb_path in shard:\n",
    "        with open(nb_path) as f:\n",
    "            nb = json.load(f)\n",
    "            for cell in nb[\"cells\"]:\n",
    "                if cell[\"cell_type\"] == \"markdown\":\n",
    "                    for line in cell[\"source\"]:\n",
    "                        for token in line.strip(\"# \").lower().split():\n",
    "                            token = token.strip(punct)\n",
    "                            wc[token] += 1\n",
    "\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass each of the shards to a remote function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_items = list(Path(\".\").glob(\"ex_*.ipynb\"))\n",
    "\n",
    "iter3 = ray.util.iter.from_items(nb_items, num_shards=3)\n",
    "\n",
    "work = [nb_word_count.remote(shard) for shard in iter3.shards()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the end results, we'll aggregate the word counts calculated from each shard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_sum = defaultdict(int)\n",
    "\n",
    "for wc in ray.get(work):\n",
    "    for token, count in wc.items():\n",
    "        wc_sum[token] += count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then list the tokens ranked in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token, count in sorted(wc_sum.items(), key=lambda item: item[1], reverse=True):\n",
    "    if count > 1:\n",
    "        ic(token, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, shutdown Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel iterators provide a somewhat higher-level abstraction which uses Ray actors and `ray.wait` loops, and fit conveninently into efficient software patterns in Python.\n",
    "\n",
    "Engineering trade-offs are available a multiple levels:\n",
    "\n",
    "  * trade-off compute and memory requirements by partitioning sequences of items into data shards\n",
    "  * trade-off compute and memory requirements for transformations on items by passing the data shards to remote functions (stateless) and remote methods (stateful)\n",
    "  * trade-off the semantic guarantees on fetch ordering by using asynchronous iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
