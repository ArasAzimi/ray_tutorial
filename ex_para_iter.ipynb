{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Guided Tour of Ray Core: Parallel Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*Parallel Iterators*](https://docs.ray.io/en/latest/iter.html) provide a simple yet powerful API for data ingest and stream processing, where transformations are based on method chaining.\n",
    "\n",
    "Parallel iterators get partitioned into *data shards*, and Ray creates a worker (an *actor*) to produces the data for each shard.\n",
    "Evaluation is *lazy*, i.e., only executed when the application calls `next()` to fetch the next item in a sequence.\n",
    "\n",
    "Parallel iterators are fully serializable, so they can be passed to remote tasks and actors.\n",
    "In effect, these can be used to operate over infinite sequences of items, with the processing distributed across a cluster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start Ray…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "import logging\n",
    "import ray\n",
    "\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    logging_level=logging.ERROR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a parallel iterator from the sequence `items`, using 2 worker actors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [1, 2, 3, 4, 5]\n",
    "\n",
    "iter1 = ray.util.iter.from_items(items, num_shards=2)\n",
    "iter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `iter1` object can now be passed (i.e., serialized) to remote tasks and remote methods.\n",
    "\n",
    "To read elements from a parallel iterator, it can be converted to a [`LocalIterator`](https://docs.ray.io/en/latest/iter.html#ray.util.iter.LocalIterator) using two approaches.\n",
    "\n",
    "Calling [`gather_sync()`](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIterator.gather_sync)\n",
    "returns a local iterable for *synchronous* iteration.\n",
    "In other words, next items will be fetched from the shards on-demand as the application steps through the iterator sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_iter1 = iter1.gather_sync()\n",
    "local_iter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in local_iter1:\n",
    "    ic(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying a function to the sequence (i.e., some kind of transformation) a parallel iterator provides semantic guarantees for *fetch ordering*. In other words, the transformation is guaranteed to get applied to each element of the sequence before the next item is fetched from the source actor.\n",
    "For example, this can be useful if you need to update the source actor between iterator steps.\n",
    "\n",
    "To illustrate a simple case of how to apply a function, first we'll define a class to perform some calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CumulativeSum:\n",
    "    def __init__ (self):\n",
    "        self.total = 0\n",
    "\n",
    "    def __call__ (self, x):\n",
    "        self.total += x\n",
    "        return (self.total, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply that class to the sequence of items, using the [`for_each()`](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIterator.for_each) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in iter1.for_each(CumulativeSum()).gather_sync():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, calling [`gather_async()`](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIterator.gather_async)\n",
    "returns a local iterable for *asynchronous* iteration.\n",
    "In other words, next items will be fetched from the shards asynchronously as soon as the previous item gets computed.\n",
    "In this case, the fetch ordering only applies per shard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to access a parallel iterator is as a collection of its shards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1.shards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shard in iter1.shards():\n",
    "    ic(shard)\n",
    "    \n",
    "    for item in shard:\n",
    "        ic(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each shard should only be read by one process at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a more extended example, let's iterate through the JSON source for each of the Jupyter notebooks in this repo – roughly speaking, as if that were a streaming input source.\n",
    "\n",
    "First, we'll load the JSON files as remote objects, saving their object references in a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "nb_items = [\n",
    "    ray.put(nb_path.read_text())\n",
    "    for nb_path in Path(\".\").glob(\"ex_*.ipynb\")\n",
    "]\n",
    "\n",
    "nb_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a parallel iterator, to distribute processing across a cluster – or merely across the processor cores on your laptop.\n",
    "\n",
    "For each remote object, the following will:\n",
    "\n",
    "  * parse the text as JSON\n",
    "  * extract the text from markdown cells\n",
    "  * count the number of lines that contain the string `\"Ray\"`\n",
    "  * apply a sliding window based on `window_width`, to simulate streamed input\n",
    "  * calculate the average number of references to Ray in each notebook\n",
    "\n",
    "In other words, the processing gets evaluated in batches, one for each \"window\" of input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "window_width = 20\n",
    "\n",
    "iter2 = (\n",
    "    ray.util.iter.from_items(nb_items, num_shards=3)\n",
    "        .for_each(lambda obj_ref: json.loads(ray.get(obj_ref)))\n",
    "        .for_each(lambda nb: nb[\"cells\"])\n",
    "        .flatten()\n",
    "        .for_each(lambda cell: cell[\"source\"] if cell[\"cell_type\"] == \"markdown\" else [])\n",
    "        .flatten()\n",
    "        .for_each(lambda line: 1 if \"Ray\" in line else 0)\n",
    "        .batch(window_width)\n",
    "        .for_each(np.mean)\n",
    ")\n",
    "\n",
    "iter2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the probability of the term `Ray` occurring within the lines in each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for freq in iter2.gather_async():\n",
    "    ic(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rework this to show an example of passing iterator shards to remote functions.\n",
    "We'll define a remote function `nb_word_count` to tally *word count* among the markdown cells in each notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "@ray.remote\n",
    "def nb_word_count (shard):\n",
    "    wc = defaultdict(int)\n",
    "    punct = \"\"\"'`<>[](){}*.,:…-'\"\"\"\n",
    "    \n",
    "    stop_words = {\n",
    "        \"a\", \"and\", \"as\", \"be\", \"for\", \"from\", \"in\", \"is\", \"of\", \"on\", \"that\", \"the\", \"this\", \"to\",\n",
    "    }\n",
    "    \n",
    "    for obj_ref in shard:\n",
    "        nb = json.loads(ray.get(obj_ref))\n",
    "        \n",
    "        for cell in nb[\"cells\"]:\n",
    "            if cell[\"cell_type\"] == \"markdown\":\n",
    "                for line in cell[\"source\"]:\n",
    "                    for token in line.strip(\"# \").lower().split():\n",
    "                        token = token.strip(punct)\n",
    "\n",
    "                        if len(token) > 0 and token not in stop_words:\n",
    "                            wc[token] += 1\n",
    "\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass each of the shards to a remote function, along with the stop words object reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iter3 = ray.util.iter.from_items(nb_items, num_shards=3)\n",
    "\n",
    "work = [nb_word_count.remote(shard) for shard in iter3.shards()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the end results, we'll aggregate the word counts calculated from each shard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_sum = defaultdict(int)\n",
    "\n",
    "for wc in ray.get(work):\n",
    "    for token, count in wc.items():\n",
    "        wc_sum[token] += count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then list the tokens ranked in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token, count in sorted(wc_sum.items(), key=lambda item: item[1], reverse=True):\n",
    "    if count > 1:\n",
    "        ic(token, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, shutdown Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel iterators provide a somewhat higher-level abstraction which uses Ray actors and `ray.wait` loops, and fit conveninently into efficient software patterns in Python.\n",
    "\n",
    "Engineering trade-offs are available a multiple levels:\n",
    "\n",
    "  * trade-off compute and memory requirements by partitioning sequences of items into data shards\n",
    "  * trade-off compute and memory requirements for transformations on items by passing the data shards to remote functions (stateless) and remote methods (stateful)\n",
    "  * trade-off the semantic guarantees on fetch ordering by using asynchronous iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
